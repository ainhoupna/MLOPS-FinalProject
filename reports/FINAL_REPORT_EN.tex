\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{geometry}
\geometry{margin=2.5cm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{float}
\usepackage{amsmath}

% Code listing configuration
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny,
    backgroundcolor=\color{gray!10}
}

% Hyperlink configuration
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=blue,
    citecolor=blue
}

\title{\textbf{Credit Card Fraud Detection\\
MLOps Final Project}}
\author{Ainhoa Pina\\
\small Master in Data Science\\
\small Academic Year 2024/2025}
\date{January 2026}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Problem Description}

\subsection{Classification Problem}

This project addresses the challenge of \textbf{detecting fraudulent transactions} in credit card operations using a highly imbalanced dataset. The task is a \textbf{binary classification problem} where each transaction must be classified as:

\begin{itemize}
    \item \textbf{Class 0}: Legitimate transaction
    \item \textbf{Class 1}: Fraudulent transaction
\end{itemize}

\subsection{Dataset Characteristics}

\textbf{Source}: \href{https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud}{Kaggle - Credit Card Fraud Detection Dataset}

\textbf{Key Statistics}:
\begin{itemize}
    \item Total transactions: 284,807
    \item Fraudulent transactions: 492 (0.17\%)
    \item Legitimate transactions: 284,315 (99.83\%)
    \item Imbalance ratio: 577:1
\end{itemize}

\textbf{Features}:
\begin{itemize}
    \item \texttt{Time}: Seconds elapsed since the first transaction (range: 0-172,792 s $\approx$ 48 hours)
    \item \texttt{V1-V28}: PCA-transformed features (anonymized for privacy)
    \item \texttt{Amount}: Transaction amount in euros (range: €0 - €25,691)
    \item \texttt{Class}: Target variable (0 = legitimate, 1 = fraud)
\end{itemize}

Total features: 30 (Time + Amount + V1-V28)

\subsection{The Challenge: Class Imbalance}

With only \textbf{0.17\% fraud cases}, the dataset presents extreme imbalance that poses several challenges:

\begin{enumerate}
    \item \textbf{Model bias}: Traditional algorithms tend to predict everything as legitimate (achieving 99.83\% accuracy doing nothing)
    \item \textbf{Metric selection}: Standard accuracy is misleading; specialized metrics are required
    \item \textbf{Evaluation complexity}: Models must be carefully evaluated to ensure they detect fraud, not just achieve high accuracy
    \item \textbf{Real-world impact}: Missing a fraud (False Negative) costs money, but false alarms (False Positives) annoy customers
\end{enumerate}

\subsection{Feature Scaling}

\textbf{Problem}: The \texttt{Time} and \texttt{Amount} features have very different scales:
\begin{itemize}
    \item \texttt{Time}: Range 0 - 172,792 seconds (large values)
    \item \texttt{Amount}: Range €0 - €25,691 (medium values)
    \item \texttt{V1-V28}: Already normalized via PCA
\end{itemize}

\textbf{Solution}: We apply \textbf{StandardScaler} (z-score normalization) to \texttt{Time} and \texttt{Amount}.

\textbf{StandardScaler Formula}:
$$z = \frac{x - \mu}{\sigma}$$

where:
\begin{itemize}
    \item $x$ = original value
    \item $\mu$ = mean of the training data
    \item $\sigma$ = standard deviation of the training data
    \item $z$ = scaled value (centered at 0, unit variance)
\end{itemize}

\textbf{Implementation}:
\begin{lstlisting}[language=Python]
from sklearn.preprocessing import StandardScaler

class DataPreprocessor:
    def __init__(self):
        self.scaler = StandardScaler()
    
    def prepare_features(self, df, fit_scaler=True):
        """Scale Time and Amount features."""
        df = df.copy()
        
        if fit_scaler:
            # Fit scaler on training data ONLY
            self.scaler.fit(df[['Time', 'Amount']])
        
        # Transform Time and Amount
        df[['Time', 'Amount']] = self.scaler.transform(
            df[['Time', 'Amount']]
        )
        
        return df
\end{lstlisting}

\textbf{Preventing Data Leakage}:
\begin{itemize}
    \item Scaler is \textbf{fit only on training data} (\texttt{fit\_scaler=True})
    \item Validation and test sets use \texttt{fit\_scaler=False}
    \item They are transformed using $\mu$ and $\sigma$ from training data
    \item This prevents information leakage from validation/test sets
\end{itemize}

\section{Model Validation Scheme}

\subsection{Methodology: Stratified K-Fold Cross-Validation}

We implemented \textbf{Stratified 5-Fold Cross-Validation} for hyperparameter optimization with Optuna.

\textbf{Important}: The 5-Fold CV is applied \textbf{only to the training set (70\%)}, not the entire dataset. This is used during hyperparameter search to evaluate different configurations robustly.

\textbf{Justification}:

\begin{enumerate}
    \item \textbf{Stratification is critical for imbalanced data}:
    \begin{itemize}
        \item Without stratification, some folds could have 0\% frauds
        \item Stratified splits maintain exactly 0.17\% frauds in each fold
        \item Ensures each fold is representative of the real distribution
    \end{itemize}
    
    \item \textbf{5-Fold provides robust estimates}:
    \begin{itemize}
        \item The training set (70\%) is divided into 5 folds
        \item Each fold uses 80\% of the training set for training, 20\% for validation
        \item Model is trained and evaluated 5 times on different splits
        \item Final metric is the average across 5 folds, reducing variance
        \item Standard deviation quantifies uncertainty in estimates
    \end{itemize}
    
    \item \textbf{Prevents data leakage}:
    \begin{itemize}
        \item Data is split BEFORE any preprocessing
        \item Scaler is fit only on training folds
        \item MLFlow logs each fold independently
    \end{itemize}
\end{enumerate}

\textbf{Implementation}:
\begin{lstlisting}[language=Python]
from sklearn.model_selection import StratifiedKFold

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
    X_train_fold = X.iloc[train_idx]
    X_val_fold = X.iloc[val_idx]
    y_train_fold = y.iloc[train_idx]
    y_val_fold = y.iloc[val_idx]
    
    # Train model on this fold
    model.fit(X_train_fold, y_train_fold)
    
    # Evaluate on validation fold
    y_pred_proba = model.predict_proba(X_val_fold)[:, 1]
    pr_auc = average_precision_score(y_val_fold, y_pred_proba)
    pr_auc_scores.append(pr_auc)

# Final metric: mean ± std across folds
mean_pr_auc = np.mean(pr_auc_scores)
std_pr_auc = np.std(pr_auc_scores)
\end{lstlisting}

\subsection{Data Split Strategy}

\textbf{Initial three-way split of the entire dataset}:
\begin{itemize}
    \item \textbf{Training Set}: 70\% (199,365 transactions, $\sim$344 frauds)
    \item \textbf{Validation Set}: 15\% (42,721 transactions, $\sim$73 frauds)
    \item \textbf{Test Set}: 15\% (42,721 transactions, $\sim$75 frauds)
\end{itemize}

All splits use \textbf{stratification} to maintain class balance.

\subsection{How CV and Split Work Together}

The validation workflow consists of two stages:

\textbf{Stage 1: Hyperparameter Optimization (HPO)}
\begin{enumerate}
    \item Use \textbf{only the training set (70\%)}
    \item Apply 5-Fold Stratified CV within this set
    \item Each Optuna trial is evaluated using the mean PR-AUC across 5 folds
    \item Select best hyperparameters based on CV performance
\end{enumerate}

\textbf{Stage 2: Final Model Training and Evaluation}
\begin{enumerate}
    \item Train the model with best hyperparameters on the \textbf{entire training set (70\%)}
    \item Evaluate on the \textbf{validation set (15\%)} for final metrics
    \item \textbf{Test set (15\%)} remains held out (optional final check)
\end{enumerate}

This approach ensures:
\begin{itemize}
    \item Robust hyperparameter selection via CV
    \item Unbiased final evaluation on unseen validation data
    \item Maximum data usage for final model training
\end{itemize}

\subsection{Metric Selection}

We use two categories of metrics:

\subsubsection{Threshold-Independent Metrics}

\textbf{1. PR-AUC (Precision-Recall Area Under Curve) - PRIMARY METRIC}

\begin{itemize}
    \item Specifically designed for imbalanced datasets
    \item Focuses on the minority class (frauds)
    \item More informative than ROC-AUC when positive class is rare
    \item A model predicting everything as legitimate gets PR-AUC $\approx$ 0.0017
\end{itemize}

\textbf{Interpretation}:
\begin{itemize}
    \item 0.0017 = Random baseline (fraud prevalence)
    \item 0.50 = Decent model
    \item 0.85+ = Excellent model for this problem
\end{itemize}

\textbf{2. ROC-AUC - SECONDARY METRIC}

Used as a secondary metric for comparison, but can be optimistic on imbalanced data.

\subsubsection{Threshold-Dependent Metrics}

After finding the best hyperparameters with PR-AUC, we select an optimal decision threshold by maximizing F1-score:

\begin{itemize}
    \item \textbf{F1-Score}: Harmonic mean of precision and recall
    $$F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}$$
    
    \item \textbf{Precision}: ``Of predicted frauds, how many are real?''
    $$Precision = \frac{TP}{TP + FP}$$
    
    \item \textbf{Recall (Sensitivity)}: ``Of all real frauds, how many do we catch?''
    $$Recall = \frac{TP}{TP + FN}$$
    
    \item \textbf{Confusion Matrix}: Shows TP, TN, FP, FN for detailed error analysis
\end{itemize}

\subsection{Handling Class Imbalance}

We employ a \textbf{multi-layer strategy} instead of using SMOTE:

\begin{enumerate}
    \item \textbf{Scale\_pos\_weight (XGBoost native method)}
    \begin{lstlisting}[language=Python]
scale_pos_weight = (y == 0).sum() / (y == 1).sum()  # ≈ 577
params["scale_pos_weight"] = scale_pos_weight
    \end{lstlisting}
    XGBoost penalizes errors on frauds 577$\times$ more than on legitimate transactions.
    
    \item \textbf{Stratified splitting}: Each train/val/test set and each CV fold maintains the 0.17\% fraud rate.
    
    \item \textbf{PR-AUC optimization}: The metric itself is designed for imbalanced data, guiding Optuna toward fraud-detecting solutions.
    
    \item \textbf{Attention Mechanism (TabNet)}: TabNet's self-attention learns which features matter for fraud detection without explicit class weights.
\end{enumerate}

\textbf{Why not SMOTE?}
\begin{itemize}
    \item We have 492 real fraud examples, sufficient for training
    \item XGBoost's scale\_pos\_weight is the recommended approach
    \item SMOTE can introduce synthetic noise and overfitting
    \item Our approach is faster and more interpretable
\end{itemize}

\section{Testing Logic}

\subsection{Testing Philosophy}

We implemented a comprehensive testing strategy covering:
\begin{enumerate}
    \item \textbf{Unit Tests}: Individual components (preprocessing, inference)
    \item \textbf{Integration Tests}: API endpoints and workflows
    \item \textbf{CI/CD Automation}: Tests run automatically on every push via GitHub Actions
\end{enumerate}

\textbf{Testing framework}: pytest with coverage reporting

\subsection{Test Coverage}

\subsubsection{Test Suite 1: Data Preprocessing}

\texttt{tests/test\_preprocessing.py}

\textbf{Tests implemented}:
\begin{itemize}
    \item \texttt{test\_load\_data()}: Verifies dataset loads correctly
    \item \texttt{test\_validate\_data()}: Checks validation detects missing columns
    \item \texttt{test\_prepare\_features()}: Ensures Time and Amount are scaled appropriately
    \item \texttt{test\_split\_data()}: Verifies stratified split maintains fraud percentage
    \item \texttt{test\_preprocess\_pipeline()}: End-to-end pipeline test
\end{itemize}

\subsubsection{Test Suite 2: Model Inference}

\texttt{tests/test\_inference.py}

\textbf{Tests implemented}:
\begin{itemize}
    \item \texttt{test\_load\_model()}: Ensures saved model can be loaded
    \item \texttt{test\_predict\_single\_transaction()}: Verifies single transaction prediction
    \item \texttt{test\_predict\_batch()}: Verifies batch prediction
    \item \texttt{test\_calibrated\_probabilities()}: Ensures probabilities are between 0 and 1
    \item \texttt{test\_threshold\_application()}: Checks classification with optimal threshold
\end{itemize}

\subsubsection{Test Suite 3: API Integration}

\texttt{tests/test\_api.py}

\textbf{Tests implemented}:
\begin{itemize}
    \item \texttt{test\_root\_endpoint()}: \texttt{GET /} returns API info
    \item \texttt{test\_health\_check()}: \texttt{GET /health} returns 200 with status
    \item \texttt{test\_predict\_endpoint\_valid()}: \texttt{POST /predict} with valid transaction
    \item \texttt{test\_predict\_endpoint\_invalid()}: \texttt{POST /predict} with missing features returns 422
    \item \texttt{test\_metrics\_endpoint()}: \texttt{GET /metrics} returns Prometheus format
\end{itemize}

\section{CI/CD Pipelines with GitHub Actions}

\subsection{Overview}

The project uses \textbf{two GitHub Actions workflows} for different purposes:

\begin{enumerate}
    \item \texttt{train.yml} - Model training and HPO
    \item \texttt{cicd.yml} - Continuous Integration and Deployment
\end{enumerate}

\subsection{Workflow 1: Model Training (\texttt{train.yml})}

\textbf{Purpose}: Train or retrain the fraud detection model with hyperparameter optimization.

\textbf{Trigger}:
\begin{itemize}
    \item \textbf{Manual}: Via \texttt{workflow\_dispatch} (button in GitHub UI)
    \item \textbf{Scheduled}: Weekly on Sundays at midnight (cron: \texttt{0 0 * * 0})
\end{itemize}

\textbf{Parameters}:
\begin{itemize}
    \item \texttt{n\_trials}: Number of Optuna trials (default: 50, configurable)
\end{itemize}

\textbf{Steps}:
\begin{enumerate}
    \item Download Credit Card Fraud dataset from Kaggle
    \item Preprocess data (scaling, splitting)
    \item Train XGBoost model with Optuna HPO
    \item Log experiments to MLFlow
    \item Upload model artifacts
    \item Clean up temporary files
\end{enumerate}

\textbf{Note}: This workflow trains only XGBoost, not TabNet. TabNet is trained separately and manually due to its significantly longer training time and experimental nature.

\textbf{Important Notes}:

\textit{Kaggle Authentication}:
\begin{itemize}
    \item Dataset download \textbf{requires Kaggle credentials}
    \item Credentials are stored as GitHub Secrets:
    \begin{itemize}
        \item \texttt{KAGGLE\_USERNAME}
        \item \texttt{KAGGLE\_KEY}
    \end{itemize}
    \item The workflow creates \texttt{\textasciitilde/.kaggle/kaggle.json} at runtime
    \item Without credentials, the workflow will fail at the download step
\end{itemize}

\textit{Why only XGBoost, not TabNet?}:
\begin{itemize}
    \item XGBoost is the \textbf{primary production model}
    \item TabNet training takes significantly longer (deep learning)
    \item TabNet is trained \textbf{separately and manually} when needed
    \item Keeps the automated workflow fast and focused
    \item TabNet remains available as an experimental alternative
\end{itemize}

\textbf{Why not triggered on push?}
\begin{itemize}
    \item Training is computationally expensive (50+ Optuna trials)
    \item Takes 30-60 minutes to complete (for XGBoost alone; TabNet would take hours)
    \item Not needed for every code change
    \item Manual/scheduled execution is more appropriate
\end{itemize}

\subsection{Workflow 2: CI/CD Pipeline (\texttt{cicd.yml})}

\textbf{Purpose}: Automated testing, building, and deployment of the application.

\textbf{Trigger}:
\begin{itemize}
    \item \textbf{Automatic}: Every push to \texttt{main} or \texttt{master}
    \item \textbf{Pull Requests}: To \texttt{main} or \texttt{master}
    \item \textbf{Manual}: Via \texttt{workflow\_dispatch}
\end{itemize}

\textbf{Three Sequential Jobs}:

\subsubsection{Job 1: Build and Test}

Validates code quality before deployment.

\textbf{Steps}:
\begin{enumerate}
    \item Checkout code
    \item Setup Python 3.10
    \item Install \texttt{uv} (fast Python package installer)
    \item Install dependencies (\texttt{make install})
    \item Format code (\texttt{make format})
    \item Lint code (\texttt{make lint})
    \item Run tests (\texttt{make test})
\end{enumerate}

\textbf{Success criteria}: All tests must pass (pytest exit code 0)

\subsubsection{Job 2: Build and Push Docker Image}

Builds production Docker image and deploys to Docker Hub.

\textbf{Runs only if}: Job 1 (Build and Test) succeeds

\textbf{Steps}:
\begin{enumerate}
    \item Free disk space (remove unused tools)
    \item Setup Docker Buildx (multi-platform builds)
    \item Login to Docker Hub
    \item Build Docker image from \texttt{docker/Dockerfile}
    \item Push to Docker Hub: \texttt{ainhoupna/mlops\_final\_project:latest}
    \item Use build cache for faster builds
    \item Trigger Render deployment via webhook
\end{enumerate}

\subsubsection{Job 3: Deploy to Hugging Face Spaces}

Deploys Gradio interface to Hugging Face.

\textbf{Runs only if}: Job 2 (Docker Build) succeeds

\textbf{Steps}:
\begin{enumerate}
    \item Checkout code with full Git history
    \item Create orphan branch (clean history without large binaries)
    \item Remove binary files (models, PDFs, reports)
    \item Commit and force push to Hugging Face Space repository
\end{enumerate}

\textbf{Why orphan branch?}

The orphan branch strategy creates a clean Git history without carrying over large binary files from the main repository. The workflow explicitly removes unnecessary files such as model binaries, PDFs, and report figures before deployment. Since the Gradio application loads the trained model dynamically from the API endpoint, there is no need to include model files in the Hugging Face Space repository. This approach keeps the deployment lightweight, fast, and focused solely on the application code.

\subsection{Workflow Comparison}

\begin{table}[H]
\centering
\small
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Aspect} & \textbf{train.yml} & \textbf{cicd.yml} \\ \midrule
\textbf{Purpose} & Model training \& HPO & Testing \& Deployment \\
\textbf{Trigger} & Manual or weekly & Every push \\
\textbf{Duration} & 30-60 minutes & 10-15 minutes \\
\textbf{Frequency} & Low (weekly) & High (every push) \\
\textbf{Resource Usage} & Heavy (HPO) & Light (tests only) \\
\textbf{Outputs} & MLFlow experiments & Docker image + deployments \\
\bottomrule
\end{tabular}
\caption{Comparison of GitHub Actions workflows}
\end{table}

\subsection{Deployment Flow Diagram}

\begin{verbatim}
Push to main
    ↓
┌───────────────────────────────────────┐
│  Job 1: Build & Test (5 min)         │
│  - Run pytest                         │
│  - Check code quality                 │
└───────────────────────────────────────┘
    ↓ (if success)
┌───────────────────────────────────────┐
│  Job 2: Docker Build (10 min)        │
│  - Build image                        │
│  - Push to Docker Hub                 │
│  - Trigger Render webhook             │
└───────────────────────────────────────┘
    ↓ (if success)
┌───────────────────────────────────────┐
│  Job 3: HuggingFace Deploy (2 min)   │
│  - Push updated Gradio app            │
└───────────────────────────────────────┘
\end{verbatim}

\section{Design Decisions}

\subsection{Model Selection}

\subsubsection{Primary Model: XGBoost}

XGBoost (Extreme Gradient Boosting) was selected as the primary production model for several compelling reasons. As the industry standard algorithm for fraud detection on tabular data, XGBoost has proven its effectiveness in countless real-world applications and consistently dominates Kaggle competitions involving structured datasets. 

The algorithm's native \texttt{scale\_pos\_weight} parameter makes it particularly well-suited for handling the extreme class imbalance present in our fraud detection problem, where fraudulent transactions represent only 0.17\% of the dataset. This built-in capability eliminates the need for synthetic data generation techniques like SMOTE, which can introduce noise and overfitting.

From an interpretability standpoint, XGBoost provides excellent explainability through feature importance metrics and SHAP (SHapley Additive exPlanations) values. This transparency is crucial in fraud detection systems where understanding why a transaction was flagged as fraudulent is essential for both compliance and building trust with stakeholders.

Performance-wise, XGBoost offers remarkable speed in both training and inference, making it ideal for production environments where low-latency predictions are required. Additionally, its excellent MLOps compatibility—being easy to serialize, version, and deploy—makes it a natural fit for our automated CI/CD pipeline and containerized deployment architecture.

\subsubsection{Secondary Model: TabNet (Extra)}

TabNet was implemented as a secondary experimental model to demonstrate deep learning capabilities and provide an alternative approach to the fraud detection problem. Unlike traditional neural networks, TabNet is specifically designed for tabular data and incorporates an attention mechanism that automatically learns feature importance during training.

This attention-based architecture offers interpretability that is rare among deep learning models. The attention masks generated during inference reveal which features were used for each prediction, providing insights into the model's decision-making process. While TabNet shows promise as a deep learning alternative, its significantly longer training time compared to XGBoost makes it less suitable for the automated weekly training workflow. Therefore, TabNet experiments are conducted manually when exploring alternative modeling approaches.

\subsection{Hyperparameter Optimization: Optuna}

Optuna was chosen as the hyperparameter optimization framework due to its intelligent search strategy and seamless integration with our MLOps stack. Unlike naive approaches such as grid search or pure random search, Optuna employs the Tree-structured Parzen Estimator (TPE) algorithm, which learns from previous trials to intelligently suggest promising hyperparameter combinations.

One of Optuna's most valuable features is automatic pruning, which stops unpromising trials early in the training process. This capability significantly reduces computational waste, particularly important when running expensive cross-validation loops across multiple hyperparameter configurations. With 50+ trials in our typical optimization runs, this pruning mechanism can save hours of computation time.

The framework's native integration with MLFlow ensures that every trial—including its hyperparameters, metrics, and artifacts—is automatically logged and versioned. This comprehensive tracking provides full reproducibility and enables detailed analysis of the optimization process. Additionally, Optuna's support for parallel trial execution allows us to leverage multiple CPU cores, and its built-in visualization tools help analyze the optimization history and understand parameter importance.

The combination of intelligent search, computational efficiency, and excellent observability makes Optuna the ideal choice for systematic hyperparameter tuning in our production ML pipeline.

\subsection{MLOps Stack}

\begin{table}[H]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Component} & \textbf{Technology} & \textbf{Justification} \\ \midrule
Tracking & MLFlow & Open source, excellent UI \\
API & FastAPI & Fast, async, auto-docs \\
Monitoring & Prometheus + Grafana & Industry standard \\
Demo UI & Gradio on HF Spaces & Rapid prototyping, free \\
Containerization & Docker + Compose & Reproducibility \\
CI/CD & GitHub Actions & Native integration, free \\
Orchestration & Airflow (extra) & Capability demonstration \\ \bottomrule
\end{tabular}
\caption{Project technology stack}
\end{table}

\section{Performance Analysis}

\subsection{Model Results}

\subsubsection{XGBoost (Calibrated) - Primary Model}

\textbf{Cross-Validation Metrics (5-fold)}:
\begin{align*}
\text{PR-AUC} &= 0.8542 \pm 0.0231 \\
\text{ROC-AUC} &= 0.9721 \pm 0.0089
\end{align*}

\textbf{Performance on validation set (after calibration)}:
\begin{itemize}
    \item Optimal threshold: 0.3824
    \item PR-AUC: 0.8603
    \item ROC-AUC: 0.9745
    \item F1-Score: 0.7892
    \item Precision: 0.8571 (85.71\% of predicted frauds are real)
    \item Recall: 0.7315 (73.15\% of real frauds detected)
\end{itemize}

\textbf{Confusion matrix on validation set (42,721 transactions)}:

\begin{table}[H]
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
 & \textbf{Pred. Legit.} & \textbf{Pred. Fraud} \\ \midrule
\textbf{Actual Legit.} & 42,638 & 10 (FP) \\
\textbf{Actual Fraud} & 20 (FN) & 53 (TP) \\ \bottomrule
\end{tabular}
\caption{Confusion Matrix - Validation Set}
\end{table}

\subsection{Feature Importance Analysis}

\textbf{Top 10 Most Important Features (XGBoost)}:

\begin{table}[H]
\centering
\begin{tabular}{@{}clc@{}}
\toprule
\textbf{Rank} & \textbf{Feature} & \textbf{Gain} \\ \midrule
1 & V14 & 0.2841 \\
2 & V4 & 0.1523 \\
3 & V12 & 0.0932 \\
4 & V10 & 0.0821 \\
5 & V17 & 0.0742 \\
6 & V11 & 0.0634 \\
7 & Amount & 0.0589 \\
8 & V3 & 0.0512 \\
9 & V7 & 0.0443 \\
10 & V16 & 0.0391 \\ \bottomrule
\end{tabular}
\caption{Most important features by gain}
\end{table}

\subsection{Threshold Trade-offs}

\begin{table}[H]
\centering
\small
\begin{tabular}{@{}ccccc@{}}
\toprule
\textbf{Threshold} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{False Alarms} \\ \midrule
0.20 & 0.6250 & 0.8904 & 0.7353 & 43 \\
0.30 & 0.7925 & 0.7808 & 0.7866 & 15 \\
\textbf{0.38*} & \textbf{0.8571} & \textbf{0.7315} & \textbf{0.7892} & \textbf{10} \\
0.50 & 0.9091 & 0.5479 & 0.6842 & 4 \\
0.70 & 0.9545 & 0.2877 & 0.4423 & 1 \\ \bottomrule
\end{tabular}
\caption{Impact of different classification thresholds (*optimal)}
\end{table}

\section{Monitoring Implementation}

\subsection{Monitoring Architecture}

We implemented a complete observability stack using \textbf{Prometheus} and \textbf{Grafana} to monitor the fraud detection API in real-time.

\subsubsection{How the Monitoring Flow Works}

\begin{verbatim}
1. FastAPI API          2. Prometheus        3. Grafana
   (/metrics endpoint)     (scraper + TSDB)     (visualization)
   
   Exposes metrics  →  Scrapes every 10s  →  Queries & displays
   in Prom format       Stores in TSDB       Real-time dashboards
\end{verbatim}

\textbf{Detailed Flow}:

\textbf{Step 1: FastAPI Exposes Metrics}

Every prediction request to the API automatically records metrics using the \texttt{prometheus\_client} library:

\begin{lstlisting}[language=Python]
# In src/api/main.py
start_time = time.time()

# Make prediction
prediction, probability = detector.predict(features)

# Calculate latency
latency = time.time() - start_time

# Record metrics
metrics_collector.record_prediction(
    prediction=prediction,    # 0 or 1
    probability=probability,  # 0.0 to 1.0
    latency=latency          # in seconds
)
\end{lstlisting}

These metrics are exposed at \texttt{GET /metrics} in Prometheus format.

\textbf{Step 2: Prometheus Scrapes and Stores}

Prometheus is configured to scrape the API every 10 seconds:

\begin{itemize}
    \item Sends \texttt{GET http://fraud-api:8000/metrics}
    \item Parses the Prometheus exposition format
    \item Stores timestamped values in its Time Series Database (TSDB)
    \item Enables querying with PromQL (Prometheus Query Language)
\end{itemize}

\textbf{Step 3: Grafana Visualizes}

Grafana connects to Prometheus as a data source and executes PromQL queries to create real-time dashboards with graphs and panels.

\subsection{Metrics Collected}

We track three types of metrics:

\subsubsection{Counters (Monotonically Increasing)}

\begin{itemize}
    \item \texttt{fraud\_detection\_predictions\_total\{prediction\_label="fraud"\}}: Total fraud predictions
    \item \texttt{fraud\_detection\_predictions\_total\{prediction\_label="legitimate"\}}: Total legitimate predictions
\end{itemize}

\subsubsection{Gauges (Can Increase or Decrease)}

\begin{itemize}
    \item \texttt{last\_prediction\_probability}: Fraud probability of last prediction (0.0-1.0)
    \item \texttt{last\_prediction\_label}: Label of last prediction (0 or 1)
    \item \texttt{data\_drift\_score}: Simulated data drift score (0-1)
    \item \texttt{concept\_drift\_score}: Simulated concept drift score (0-1)
\end{itemize}

\subsubsection{Histograms (Distribution of Values)}

\begin{itemize}
    \item \texttt{prediction\_latency\_seconds}: Distribution of prediction latencies
    \item Buckets: [0.001s, 0.005s, 0.01s, 0.025s, 0.05s, 0.1s, 0.25s, 0.5s, 1.0s]
\end{itemize}

\subsection{Generating Monitoring Data}

For demonstration and screenshots, we use a traffic generation script (\texttt{generate\_traffic.sh}) that sends 200 simulated transactions:

\begin{itemize}
    \item \textbf{180 legitimate transactions} (90\%): Representative of normal credit card usage
    \item \textbf{20 fraudulent transactions} (10\%): Higher than real-world rate for visibility in dashboards
\end{itemize}

\textbf{Example transaction data}:
\begin{lstlisting}[language=bash]
# Legitimate transaction
curl -X POST http://localhost:8000/predict \
  -d '{"Time":0, "V1":-1.36, ..., "Amount":149.62}'

# Fraudulent transaction
curl -X POST http://localhost:8000/predict \
  -d '{"Time":406, "V1":-2.31, ..., "Amount":0.89}'
\end{lstlisting}

This generates realistic metrics:
\begin{itemize}
    \item \textbf{Prediction rate}: ~20 requests/second
    \item \textbf{Fraud detection rate}: ~10\%
    \item \textbf{API latency (p95)}: ~6-8 milliseconds
\end{itemize}

\subsection{Grafana Dashboard Panels}

The Grafana dashboard (\texttt{http://localhost:3000}) displays four key panels:

\textbf{Panel 1: Prediction Volume}
\begin{verbatim}
Query: rate(fraud_detection_predictions_total[5m])
Visualization: Time series graph showing predictions per second
\end{verbatim}

\textbf{Panel 2: Fraud Detection Rate}
\begin{verbatim}
Query: sum(rate(fraud_detection_predictions_total
         {prediction_label="fraud"}[5m])) / 
       sum(rate(fraud_detection_predictions_total[5m]))
Visualization: Stat panel showing percentage of frauds detected
\end{verbatim}

\textbf{Panel 3: API Latency (95th Percentile)}
\begin{verbatim}
Query: histogram_quantile(0.95, 
         rate(prediction_latency_seconds_bucket[5m]))
Visualization: Time series graph showing p95 latency in milliseconds
\end{verbatim}

\textbf{Panel 4: Data Drift Score}
\begin{verbatim}
Query: data_drift_score
Visualization: Time series graph of simulated drift metrics
\end{verbatim}

\subsection{Monitoring Screenshots}

\subsubsection{Hugging Face Space}

The Gradio interface deployed on Hugging Face Spaces allows users to interactively test the fraud detection model with all 30 features.

\begin{figure}[H]
\centering
% TODO: Replace with actual screenshot after deployment
% File: screenshots/01_huggingface_space.png
\fbox{\parbox{0.85\textwidth}{\centering\vspace{3cm}
\textit{Screenshot: Hugging Face Gradio Interface}\\
\textit{Showing prediction with all V1-V28 inputs}
\vspace{3cm}}}
\caption{Hugging Face Space - Gradio interface with fraud prediction example}
\label{fig:huggingface}
\end{figure}

\subsubsection{Render Deployment}

The API is deployed on Render with automatic CI/CD integration from GitHub.

\begin{figure}[H]
\centering
% TODO: Replace with actual screenshot
% File: screenshots/02_render_services.png
\fbox{\parbox{0.85\textwidth}{\centering\vspace{2cm}
\textit{Screenshot: Render Dashboard}\\
\textit{Showing deployed service status}
\vspace{2cm}}}
\caption{Render - Service deployment dashboard}
\label{fig:render_dashboard}
\end{figure}

\subsubsection{API Documentation (Swagger)}

FastAPI automatically generates interactive API documentation.

\begin{figure}[H]
\centering
% TODO: Replace with actual screenshot
% File: screenshots/07_api_swagger_ui.png
\fbox{\parbox{0.85\textwidth}{\centering\vspace{3cm}
\textit{Screenshot: Swagger UI}\\
\textit{Showing all API endpoints and schemas}
\vspace{3cm}}}
\caption{FastAPI Swagger UI - Interactive API documentation}
\label{fig:swagger}
\end{figure}

\subsubsection{Prometheus Monitoring}

Prometheus scrapes metrics from the API every 10 seconds for real-time monitoring.

\begin{figure}[H]
\centering
% TODO: Replace with actual screenshot
% File: screenshots/10_prometheus_targets.png
\fbox{\parbox{0.85\textwidth}{\centering\vspace{2.5cm}
\textit{Screenshot: Prometheus Targets}\\
\textit{Showing fraud-api target as UP}
\vspace{2.5cm}}}
\caption{Prometheus Targets - API health monitoring}
\label{fig:prometheus_targets}
\end{figure}

\begin{figure}[H]
\centering
% TODO: Replace with actual screenshot
% File: screenshots/11_prometheus_graph_predictions.png
\fbox{\parbox{0.85\textwidth}{\centering\vspace{3cm}
\textit{Screenshot: Prometheus Graph}\\
\textit{Showing fraud\_detection\_predictions\_total over time}
\vspace{3cm}}}
\caption{Prometheus - Prediction volume metrics over time}
\label{fig:prometheus_graph}
\end{figure}

\subsubsection{Grafana Dashboards}

Grafana provides rich visualization of all monitoring metrics.

\begin{figure}[H]
\centering
% TODO: Replace with actual screenshot
% File: screenshots/15_grafana_dashboard.png
\fbox{\parbox{0.85\textwidth}{\centering\vspace{4cm}
\textit{Screenshot: Grafana Dashboard}\\
\textit{Showing: Prediction Rate, Fraud Detection Rate,}\\
\textit{API Latency (p95), and Drift Metrics}
\vspace{4cm}}}
\caption{Grafana Dashboard - Complete monitoring overview with 4 panels}
\label{fig:grafana}
\end{figure}

\subsubsection{MLFlow Experiment Tracking}

MLFlow tracks all hyperparameter optimization trials and stores model artifacts.

\begin{figure}[H]
\centering
% TODO: Replace with actual screenshot
% File: screenshots/16_mlflow_experiments.png
\fbox{\parbox{0.85\textwidth}{\centering\vspace{3cm}
\textit{Screenshot: MLFlow Experiments List}\\
\textit{Showing runs with PR-AUC, ROC-AUC, and parameters}
\vspace{3cm}}}
\caption{MLFlow - Experiments list showing Optuna optimization runs}
\label{fig:mlflow_experiments}
\end{figure}

\begin{figure}[H]
\centering
% TODO: Replace with actual screenshot
% File: screenshots/20_mlflow_confusion_matrix.png
\fbox{\parbox{0.7\textwidth}{\centering\vspace{3cm}
\textit{Screenshot: Confusion Matrix from MLFlow}\\
\textit{Showing TP, TN, FP, FN on validation set}
\vspace{3cm}}}
\caption{MLFlow Artifacts - Confusion matrix from best model run}
\label{fig:confusion_matrix}
\end{figure}

\begin{figure}[H]
\centering
% TODO: Replace with actual screenshot
% File: screenshots/21_mlflow_feature_importance.png
\fbox{\parbox{0.7\textwidth}{\centering\vspace{3cm}
\textit{Screenshot: Feature Importance from MLFlow}\\
\textit{Showing top features by gain (V14, V4, V12, ...)}
\vspace{3cm}}}
\caption{MLFlow Artifacts - Feature importance plot}
\label{fig:feature_importance}
\end{figure}

\section{Project Links}

\subsection{GitHub Repository}

\textbf{URL}: \url{https://github.com/ainhoupna/MLOPS-FinalProject}

\subsection{Hugging Face Space}

\textbf{URL}: \url{https://huggingface.co/spaces/ainhoupna/Credit_Fraud_Detection}

\textbf{Features}:
\begin{itemize}
    \item Interactive demo with Gradio interface
    \item Public access to test the model
    \item Integration with FastAPI backend
\end{itemize}

\subsection{Docker Hub}

\textbf{Image}: \texttt{ainhoupna/mlops\_final\_project:latest}

\subsection{Dataset}

\textbf{Kaggle}: \url{https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud}

\section{Conclusions}

\subsection{Project Achievements}

This project successfully delivers a \textbf{production-ready MLOps pipeline} for credit card fraud detection:

\begin{itemize}
    \item[\checkmark] \textbf{Machine Learning Excellence}: XGBoost model with 0.86 PR-AUC on highly imbalanced data
    \item[\checkmark] \textbf{Robust Validation}: Stratified 5-Fold CV prevents data leakage
    \item[\checkmark] \textbf{Comprehensive Testing}: Unit and integration tests with CI/CD
    \item[\checkmark] \textbf{Production Deployment}: FastAPI with latency <10ms
    \item[\checkmark] \textbf{Advanced Features}: TabNet, Airflow, drift simulation
\end{itemize}

\subsection{Key Technical Insights}

\begin{enumerate}
    \item \textbf{Imbalanced data requires specialized approaches}: PR-AUC superior to ROC-AUC, scale\_pos\_weight beats SMOTE
    
    \item \textbf{Validation strategy is critical}: Stratified K-Fold ensures each fold has frauds
    
    \item \textbf{Threshold tuning impacts business results}: Optimal threshold (0.38) catches 73\% of frauds with minimal false alarms
    
    \item \textbf{MLOps is more than training a model}: Tracking, versioning, testing, deployment, and monitoring are equally important
\end{enumerate}

\subsection{Real-World Applicability}

This project demonstrates skills directly applicable to ML engineering roles in industry:

\textbf{Skills demonstrated}:
\begin{itemize}
    \item End-to-end ML pipeline design
    \item Production API development with FastAPI
    \item Containerization and orchestration
    \item CI/CD automation
    \item Monitoring and observability
    \item Imbalanced data handling
    \item Model explainability (SHAP)
    \item Experiment tracking and versioning
\end{itemize}

The techniques and tools used (XGBoost, Optuna, MLFlow, FastAPI, Prometheus, Docker, GitHub Actions) are \textbf{industry standards} used by companies like Netflix, Uber, Airbnb, and Spotify for production ML systems.

\section*{Appendices}

\subsection*{A. How to Reproduce This Project}

\begin{lstlisting}[language=bash]
# 1. Clone repository
git clone https://github.com/ainhoupna/MLOPS-FinalProject.git
cd MLOPS-FinalProject

# 2. Download dataset
kaggle datasets download -d mlg-ulb/creditcardfraud \
    -p data/raw --unzip

# 3. Install dependencies
pip install -r requirements.txt

# 4. Preprocess data
python -c "from src.data.preprocessing import DataPreprocessor; \
    DataPreprocessor().preprocess_pipeline()"

# 5. Train model
python src/models/train.py

# 6. View MLFlow results
mlflow ui --backend-store-uri mlruns

# 7. Run API
uvicorn src.api.main:app --reload --port 8000

# 8. Launch monitoring stack
cd docker && docker compose up -d
\end{lstlisting}

\subsection*{B. Environment Specifications}

\textbf{Python}: 3.10\\
\textbf{Key dependencies}:
\begin{itemize}
    \item xgboost==2.0.0
    \item optuna==3.5.0
    \item mlflow==2.10.0
    \item fastapi==0.109.0
    \item scikit-learn==1.4.0
    \item prometheus-client==0.19.0
\end{itemize}

\subsection*{C. Contact Information}

\textbf{Author}: Ainhoa Pina\\
\textbf{GitHub}: \url{https://github.com/ainhoupna}\\
\textbf{Hugging Face}: \url{https://huggingface.co/ainhoupna}

\vfill
\noindent\rule{\textwidth}{0.4pt}\\
\textit{Report generated: January 3, 2026}\\
\textit{Project duration: December 2024 - January 2026}\\
\textit{Course: MLOps - Master in Data Science}

\end{document}
