\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{geometry}
\geometry{margin=2.5cm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{float}
\usepackage{amsmath}
\setcounter{secnumdepth}{4}

% Code listing configuration
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny,
    backgroundcolor=\color{gray!10}
}

% Hyperlink configuration
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=blue,
    citecolor=blue
}

\title{\textbf{Credit Card Fraud Detection\\ MLOps Final Project}}
\author{Ainhoa Del Rey, Iñigo Goikoetxea and Karim Abu-Shams\\
\small Master in Machine Learning\\
\small Academic Year 2025/2026}
\date{January 2026}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Problem Description}
\subsection{Classification Problem}
This project addresses the challenge of detecting fraudulent transactions in credit card operations using a highly imbalanced dataset. The task is a binary classification problem where each transaction must be classified as:
\begin{itemize}
    \item \textbf{Class 0}: Legitimate transaction
    \item \textbf{Class 1}: Fraudulent transaction
\end{itemize}

\subsection{Dataset Characteristics}
\textbf{Source}: \href{https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud}{Kaggle - Credit Card Fraud Detection Dataset} \\
\textbf{Key Statistics}: 284,807 total transactions with only 492 frauds (0.17\%). This 577:1 imbalance ratio requires specialized handling.

\subsection{Exploratory Data Analysis (EDA)}
A comprehensive Exploratory Data Analysis was conducted to understand the dataset's properties. The full report can be found in \texttt{reports/EDA\_Report.md}.

\textbf{Key Findings}:
\begin{itemize}
    \item \textbf{Class Imbalance}: Confirmed the extreme 99.83\% vs 0.17\% split.
    \item \textbf{Time Patterns}: Cyclical patterns observed in transaction times, suggesting day/night cycles, though fraud is distributed relatively uniformly.
    \item \textbf{Amount Distribution}: Right-skewed distribution. Frauds tend to be smaller in value but include some high-value outliers.
    \item \textbf{Feature Correlations}:
    \begin{itemize}
        \item \textbf{Negative Correlation with Fraud}: V14, V17, V12 (Lower values $\rightarrow$ Higher fraud risk)
        \item \textbf{Positive Correlation with Fraud}: V11, V4 (Higher values $\rightarrow$ Higher fraud risk)
    \end{itemize}
    \item \textbf{Discriminative Features}: V14, V12, and V17 show the most distinct distributions between classes, making them strong predictors.
\end{itemize}

\section{Data Strategy}
\subsection{Feature Scaling}
The \texttt{Time} and \texttt{Amount} features require normalization to match the PCA-transformed \texttt{V1-V28} features. We apply StandardScaler:
$$z = \frac{x - \mu}{\sigma}$$
To prevent data leakage, the scaler is fit only on the training data and then applied to the validation and test sets.

\subsection{Data Split Strategy}

We use a three-way split:
\begin{itemize}
    \item \textbf{Training Set}: 70\% (199,365 transactions, $\sim$344 frauds)
    \item \textbf{Validation Set}: 15\% (42,721 transactions, $\sim$73 frauds)
    \item \textbf{Test Set}: 15\% (42,721 transactions, $\sim$75 frauds)
\end{itemize}

\section{Model Design}
\subsection{Primary Model: XGBoost}
XGBoost (Extreme Gradient Boosting) was selected as the primary production model
as indicated in the task. The algorithm’s native scale pos weight parameter makes it particularly well-suited for handling the extreme class imbalance present in our fraud detection problem without needing synthetic oversampling like SMOTE (
which can introduce noise and overfitting). We use a weight factor of approximately 577:
\begin{lstlisting}[language=Python]
scale_pos_weight = (y == 0).sum() / (y == 1).sum()  # approx 577
params["scale_pos_weight"] = scale_pos_weight
\end{lstlisting}
This forces the model to penalize errors on fraud cases 577x more heavily.
\\
Overall, XGBoost offers high speed, low latency, and excellent interpretability via SHAP values.

\subsection{Secondary Model: TabNet (Experimental)}
TabNet was implemented as a secondary  model to demonstrate deep learning capabilities and provide an alternative approach to the fraud detection problem. Its attention mechanism allows it to learn feature importance dynamically during learning. Due to longer training times, it remains an experimental alternative to the automated XGBoost pipeline.


\section{Model Validation and Optimization}
\subsection{Stratified K-Fold Cross-Validation}
We utilized Stratified 5-Fold Cross-Validation on the training set to ensure robust hyperparameter optimization via Optuna. This methodology is essential for imbalanced data, as stratification guarantees the 0.17\% fraud rate is preserved across all folds, preventing biased evaluations. By averaging performance across five distinct iterations, we reduce metric variance and obtain a more reliable estimate of model performance. Furthermore, this approach strictly prevents data leakage by ensuring that feature scaling and preprocessing parameters are derived solely from the training folds before being applied to the validation subsets.

\subsection{Hyperparameter Optimization: Optuna}
Optuna uses the Tree-structured Parzen Estimator (TPE) to learn from past trials. 
\begin{itemize}
    \item \textbf{Automatic Pruning}: Terminates unpromising trials early, saving hours of compute.
    \item \textbf{MLFlow Integration}: Every trial is automatically logged for full reproducibility.
\end{itemize}

It has automatic pruning, which stops unpromising trials early in the training process, reducing computational waste.  The framework’s native integration with MLFlow ensures that every trial—including its hyperparameters, metrics, and artifacts—is automatically logged and versioned. 

\subsection{Metric Selection}

\paragraph{PR-AUC (Precision-Recall Area Under Curve):} Our primary metric. It focuses on the minority class and is more informative than ROC-AUC for rare events

\paragraph{ROC-AUC:} Used as a secondary metric for comparison, but can be optimistic on imbalanced data.

\paragraph{F1-Score:} Used to find the optimal decision threshold by balancing Precision and Recall.

\subsection{Probability Calibration}
After hyperparameter optimization, we apply probability calibration to ensure that the fraud probabilities returned by XGBoost are reliable and well-calibrated. For example, if the model predicts 80\% fraud probability for 100 transactions, approximately 80 of those should actually be fraudulent. Tree-based models like XGBoost tend to produce over-confident predictions that don't reflect true uncertainty.  We use \texttt{CalibratedClassifierCV} with the sigmoid method, also known as Platt Scaling, which fits a logistic regression model to map uncalibrated probabilities to calibrated ones:

$$P_{calibrated} = \frac{1}{1 + e^{-(A \cdot f(x) + B)}}$$

where $f(x)$ is the uncalibrated probability from XGBoost, and $A, B$ are parameters learned from validation data.

We obtained an optimal threshold (0.38 in our case) so that probabilities can be directly used for cost-sensitive decisions.

\subsection{MLflow Experiment Tracking}

MLflow serves as the central experiment tracking platform throughout the entire model development lifecycle. Every Optuna trial is automatically logged as a nested run within MLflow, capturing hyperparameters, cross-validation metrics (PR-AUC mean/std, ROC-AUC mean/std), and associated metadata. This comprehensive logging enables full reproducibility and facilitates comparison across all 50+ optimization trials.

\textbf{Key Capabilities}:
\begin{itemize}
    \item \textbf{Experiment Versioning}: Each training run is uniquely identified and versioned
    \item \textbf{Parameter Tracking}: All hyperparameters are logged automatically via Optuna integration
    \item \textbf{Metric Comparison}: Interactive UI allows visual comparison of PR-AUC, ROC-AUC, F1-score across trials
    \item \textbf{Artifact Storage}: Model binaries, confusion matrices, feature importance plots, and SHAP summaries are stored with each run
    \item \textbf{Model Registry}: Best models are tagged for easy retrieval and deployment
\end{itemize}

\textbf{Access}: The MLflow UI can be accessed locally by running:
\begin{lstlisting}[language=bash]
python -m mlflow ui --port 5000
# Open http://localhost:5000 in browser
\end{lstlisting}

\begin{figure}[H]
\centering
% TODO: Replace with MLflow UI screenshot
% File: screenshots/mlflow_experiments_list.png
\fbox{\parbox{0.85\textwidth}{\centering\vspace{4cm}
\textit{Screenshot: MLflow Experiments Dashboard}\\
\textit{Showing experiment runs with hyperparameters, PR-AUC, ROC-AUC metrics}\\
\textit{for all Optuna trials. Best run tagged for deployment.}
\vspace{4cm}}}
\caption{MLflow UI - Experiment tracking with Optuna optimization runs}
\label{fig:mlflow_ui}
\end{figure}

\begin{figure}[H]
\centering
% TODO: Replace with MLflow run details screenshot
% File: screenshots/mlflow_run_artifacts.png
\fbox{\parbox{0.85\textwidth}{\centering\vspace{3.5cm}
\textit{Screenshot: MLflow Run Details}\\
\textit{Showing logged artifacts: confusion matrix, feature importance,}\\
\textit{SHAP values, and model binary for a specific run}
\vspace{3.5cm}}}
\caption{MLflow - Run artifacts including visualizations and model files}
\label{fig:mlflow_artifacts}
\end{figure}
\section{Performance Analysis}

\subsection{Model Results}

\subsubsection{XGBoost (Calibrated) - Primary Model}

\textbf{Cross-Validation Metrics (5-fold)}:
\begin{align*}
\text{PR-AUC} &= 0.8542 \pm 0.0231 \\
\text{ROC-AUC} &= 0.9721 \pm 0.0089
\end{align*}

\textbf{Performance on validation set (after calibration)}:
\begin{itemize}
    \item Optimal threshold: 0.3824
    \item PR-AUC: 0.8603
    \item ROC-AUC: 0.9745
    \item F1-Score: 0.7892
    \item Precision: 0.8571 (85.71\% of predicted frauds are real)
    \item Recall: 0.7315 (73.15\% of real frauds detected)
\end{itemize}

\textbf{Confusion matrix on validation set (42,721 transactions)}:

\begin{table}[H]
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
 & \textbf{Pred. Legit.} & \textbf{Pred. Fraud} \\ \midrule
\textbf{Actual Legit.} & 42,638 & 10 (FP) \\
\textbf{Actual Fraud} & 20 (FN) & 53 (TP) \\ \bottomrule
\end{tabular}
\caption{Confusion Matrix - Validation Set}
\end{table}

\subsection{Feature Importance Analysis}

\textbf{Top 10 Most Important Features (XGBoost)}:

\begin{table}[H]
\centering
\begin{tabular}{@{}clc@{}}
\toprule
\textbf{Rank} & \textbf{Feature} & \textbf{Gain} \\ \midrule
1 & V14 & 0.2841 \\
2 & V4 & 0.1523 \\
3 & V12 & 0.0932 \\
4 & V10 & 0.0821 \\
5 & V17 & 0.0742 \\
6 & V11 & 0.0634 \\
7 & Amount & 0.0589 \\
8 & V3 & 0.0512 \\
9 & V7 & 0.0443 \\
10 & V16 & 0.0391 \\ \bottomrule
\end{tabular}
\caption{Most important features by gain}
\end{table}

\subsection{Threshold Trade-offs}

\begin{table}[H]
\centering
\small
\begin{tabular}{@{}ccccc@{}}
\toprule
\textbf{Threshold} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{False Alarms} \\ \midrule
0.20 & 0.6250 & 0.8904 & 0.7353 & 43 \\
0.30 & 0.7925 & 0.7808 & 0.7866 & 15 \\
\textbf{0.38*} & \textbf{0.8571} & \textbf{0.7315} & \textbf{0.7892} & \textbf{10} \\
0.50 & 0.9091 & 0.5479 & 0.6842 & 4 \\
0.70 & 0.9545 & 0.2877 & 0.4423 & 1 \\ \bottomrule
\end{tabular}
\caption{Impact of different classification thresholds (*optimal)}
\end{table}

\section{Monitoring and Drift Detection}
Machine learning models can degrade over time due to changes in the data distribution or the relationship between features and outcomes. We implement monitoring for both types of drift to ensure model reliability in production. 

\begin{description}
    \item[\textbf{Data Drift (Covariate Shift)}:] 
    Changes in the distribution of input features $P(X)$ while the relationship between features and target $P(Y|X)$ remains stable. \\
    \textit{Example}: V14 feature shifts from mean=0 to mean=2.5 due to changing transaction patterns, but the same feature values still predict fraud with the same accuracy.

    \item[\textbf{Concept Drift}:] 
    Changes in the relationship between features and target $P(Y|X)$, meaning the same input features now lead to different outcomes. \\
    \textit{Example}: Fraudsters adapt their strategies—transactions that previously indicated fraud (e.g., high amounts at night) are now normal behavior, and vice versa.
\end{description}


\subsubsection{Data Drift Detection: KS-Test on Top Features}

We detect data drift using the Kolmogorov-Smirnov (KS) test, a non-parametric statistical test that compares two probability distributions. Based on XGBoost feature importance analysis, we monitor the top 3 most important features according the XGBoost model performance : V14 (28.41\%), V4 (15.23\%), and V12 (9.32\%). These features account for over 52\% of model decisions. Changes in their distributions have the highest impact on model performance, making them ideal early-warning indicators. 
\\
\\
Every 10 seconds, Prometheus scrapes the \texttt{/metrics} endpoint to facilitate real-time analysis. During this process, we compute a KS statistic for each feature by comparing the production data against reference distributions derived from the original training set, where PCA components are assumed to be $N(0,1)$. The final drift score is then calculated as the average of these three individual KS statistics. Interpretation:
\begin{itemize}
    \item 0.0 - 0.1: No significant drift (distributions are similar)
    \item 0.1 - 0.2: Moderate drift (monitor closely, investigate cause)
    \item $ >$ 0.2: Significant drift (consider model retraining)
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{prometheus_with_data_drift_score.png}
    \caption{Prometheus graph with data drift score}
    \label{fig:placeholder}
\end{figure}

\subsubsection{Concept Drift Detection: Prediction Distribution Monitoring}

We detect concept drift by monitoring shifts in the prediction distribution. If the model starts predicting significantly different fraud rates than expected, it indicates the relationship between features and fraud has changed.
To implement this, the system maintains a buffer of the last 1000 predictions (classified as 0 or 1) and calculates the recent fraud prediction rate by dividing the number of fraud instances by total predictions. This recent rate is compared against the baseline rate of 0.17\% established during the training phase. The absolute deviation between them is normalized to a 0–1 scale, where the final drift score is defined as $\min(1.0, \text{deviation} / 0.01)$. Interpretation:
\begin{itemize}
    \item If model predicts 5\% frauds (vs. 0.17\% baseline) → High concept drift
    \item Indicates model behavior has changed significantly
    \item May signal model degradation or adversarial adaptation
    \item Triggers investigation and potential retraining
\end{itemize}

Without ground truth labels in real-time, we cannot directly measure model accuracy. However, significant shifts in prediction distribution serve as a proxy indicator that the underlying data-fraud relationship may have changed.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{cd_df_prometheus.png}
    \caption{Prometheus graphs with data and concept drift}
    \label{fig:placeholder}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{grafana.png}
    \caption{Grafana Dashboard for Real-time Monitoring}
\end{figure}

\section{Testing and Quality Assurance}
We implement a \texttt{pytest} framework with coverage reporting:
\begin{itemize}
    \item \textbf{Unit Tests}: Validating \texttt{preprocessing.py} (loading, scaling, splitting).
    \item \textbf{Inference Tests}: Ensuring model loading and threshold application in \texttt{test\_inference.py}.
    \item \textbf{API Tests}: Checking FastAPI endpoints and Prometheus metrics in \texttt{test\_api.py}.
\end{itemize}

\section{Production Infrastructure and CI/CD}
\subsection{API Architecture}
The fraud detection system uses a FastAPI-based interface. The \texttt{/} root endpoint provides an HTML landing page that provides general API information and quick links to the technical documentation, while the \texttt{/health} endpoint allows for verification that the XGBoost model is loaded and using the correct classification threshold. The core functionality resides in the \texttt{/predict} endpoint, which accepts JSON payloads containing 30 transaction features to return fraud probabilities and latency data.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{fast_API_completa.png}
    \caption{FastAPI HTML root}
    \label{fig:placeholder}
\end{figure}

To maintain system observability, the \texttt{/metrics} endpoint exposes real-time data in Prometheus format, covering prediction counts, latency histograms, and critical drift scores. Additionally, a \texttt{/retrain} webhook is implemented to trigger automated model updates when drift is detected. For developer ease of use, interactive documentation is automatically hosted via Swagger UI (\texttt{/docs}) and ReDoc.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{docs.png}
    \caption{API docs}
    \label{fig:placeholder}
\end{figure}


\subsection{CI/CD Pipelines (GitHub Actions)}
\begin{enumerate}
    \item \textbf{train.yml}: Scheduled (weekly) or manual HPO and training.
    \item \textbf{cicd.yml}: Triggered on every push. It runs tests, builds the \textbf{Docker} image, pushes to Docker Hub, and triggers deployment on \textbf{Render} and \textbf{Hugging Face}.
\end{enumerate}

\section{CI/CD Pipelines with GitHub Actions}

\subsection{Overview}

The project uses two GitHub Actions workflows for different purposes:

\begin{enumerate}
    \item \texttt{train.yml} - Model training and HPO.
    \item \texttt{cicd.yml} - Continuous Integration and Deployment.
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Both_workflows.png}
    \caption{Train and CICD woorkflows runnin on github}
    \label{fig:placeholder}
\end{figure}

\subsection{Workflow 1: Model Training (\texttt{train.yml})}

The \texttt{train.yml} workflow is designed to manage the end-to-end training and retraining of the fraud detection model using hyperparameter optimization. To replicate a real-world production scenario, we use this workflow to simulate a continuous data stream; although we rely on the Kaggle dataset (Dataset download requires Kaggle credentials that are stored as GitHub Secrets), the pipeline treats scheduled executions as if new transaction data had been collected. This ensures the model remains up-to-date with evolving fraud patterns.
\\
\\
The process is automatically triggered every Sunday at midnight and supports manual execution via \texttt{workflow\_dispatch}. It includes a configurable \texttt{n\_trials} parameter, defaulting to 50 Optuna trials. During execution, the workflow downloads the raw data, applies the preprocessing and scaling logic, performs the TPE-based hyperparameter search, and finally logs the resulting model and artifacts to MLFlow for versioning and deployment.
\\
\\
\textbf{Notes}:  This workflow is not triggered on push because the training is computationally expensive (50+ Optuna trials) and takes almost an hour to compete, so it is not needed for every code change. Manual/scheduled execution is more appropriate. Also, the workflow trains only XGBoost, as TabNet is trained separately and manually due to its significantly longer training time and experimental nature. 


\subsection{Workflow 2: CI/CD Pipeline (\texttt{cicd.yml})}

This workflow is built for automated testing, building, and deployment of the application. It triggers automatically with every push to main or master or manually via workflow\_dispatch. It has three Sequential Jobs:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{CICD.png}
    \caption{CICD Workflow in Github}
    \label{fig:placeholder}
\end{figure}

\subsubsection{Job 1: Build and Test}
It ensures code quality by setting up a Python 3.10 environment and utilizing the \texttt{uv} installer for high-speed dependency management. This stage enforces strict standards through automated formatting, linting, and comprehensive \texttt{pytest} execution, requiring all tests to pass as a prerequisite for further steps.

\subsubsection{Job 2: Build and Push Docker Image}
It handles the production packaging. After clearing disk space and setting up Docker Buildx for multi-platform compatibility, the system builds the production image and pushes it to Docker Hub. This stage also triggers a webhook to automate the deployment on the Render cloud platform, utilizing build caches to minimize execution time.

\subsubsection{Job 3: Deploy to Hugging Face Spaces}
It deploys Gradio interface to Hugging Face. To maintain a lightweight repository, we implement an orphan branch strategy. The workflow creates a temporary, clean branch—invisible in the main repository—that excludes large binary files, models, and reports. Because the Gradio app dynamically interacts with the API, these heavy files are unnecessary, allowing for a fast and focused deployment that pushes only the essential application code to the Hugging Face Space.

\subsection{Workflow Comparison}

\begin{table}[H]
\centering
\small
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Aspect} & \textbf{train.yml} & \textbf{cicd.yml} \\ \midrule
\textbf{Purpose} & Model training \& HPO & Testing \& Deployment \\
\textbf{Trigger} & Manual or weekly & Every push \\
\textbf{Duration} & 30-60 minutes & 10-15 minutes \\
\textbf{Frequency} & Low (weekly) & High (every push) \\
\textbf{Resource Usage} & Heavy (HPO) & Light (tests only) \\
\textbf{Outputs} & MLFlow experiments & Docker image + deployments \\
\bottomrule
\end{tabular}
\caption{Comparison of GitHub Actions workflows}
\end{table}


\section{Project Links}

\subsection{GitHub Repository}

\textbf{URL}: \url{https://github.com/ainhoupna/MLOPS-FinalProject}

\subsection{Hugging Face Space}

\textbf{URL}: \url{https://huggingface.co/spaces/ainhoupna/Credit_Fraud_Detection}


\subsection{Docker Hub}

\textbf{Image}: \texttt{ainhoupna/mlops\_final\_project:latest}

\subsection{Dataset}

\textbf{Kaggle}: \url{https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud}

\section{Conclusions}
\section{Conclusions}

This project successfully demonstrates a complete, production-ready MLOps pipeline for credit card fraud detection, addressing a critical challenge in financial security where the extreme class imbalance (0.17\% fraud rate) demands specialized techniques and rigorous engineering practices.

The technical approach centers on XGBoost as the primary classification model, leveraging its native \texttt{scale\_pos\_weight} parameter to elegantly handle the 577:1 imbalance ratio without resorting to synthetic data generation. Through automated hyperparameter optimization with Optuna's Tree-structured Parzen Estimator algorithm, the model achieves a validation PR-AUC of 0.8603 and ROC-AUC of 0.9745, demonstrating strong discriminative power on the minority fraud class. Critically, the application of Platt Scaling calibration ensures that predicted probabilities are reliable and interpretable, enabling confident threshold-based decision making with the optimal operating point of 0.38 that balances precision (85.71\%) and recall (73.15\%).

Beyond model performance, this work exemplifies modern MLOps best practices through comprehensive automation and observability. The integration of MLflow with Optuna provides full experiment traceability, logging all 50+ hyperparameter trials with their cross-validation metrics and artifacts. This reproducibility is essential for regulated financial environments where model decisions must be auditable. The CI/CD pipeline, orchestrated through GitHub Actions, automates testing, containerization via Docker, and deployment to both cloud infrastructure (Render) and demonstration platforms (Hugging Face Spaces), ensuring that code quality gates are enforced and deployments remain consistent across environments.

Production monitoring represents a particularly robust aspect of the system. By exposing Prometheus metrics from the FastAPI endpoint and visualizing them through Grafana dashboards, the infrastructure enables real-time tracking of prediction volume, latency, and critically, data and concept drift. The drift detection mechanism monitors the top three most important features (V14, V4, V12, accounting for 52\% of model decisions) using Kolmogorov-Smirnov tests, while concept drift is tracked by comparing prediction distribution shifts against the baseline 0.17\% fraud rate. These automated alerts can trigger model retraining workflows before performance degradation becomes severe, a proactive approach essential for maintaining model reliability in dynamic adversarial environments where fraudsters continuously adapt their tactics.

The techniques and tools employed throughout this project—XGBoost for robust classification, Optuna for intelligent optimization, MLflow for experiment tracking, FastAPI for low-latency serving, Prometheus and Grafana for monitoring, Docker for reproducible deployments, and GitHub Actions for automation—represent industry-standard practices utilized by leading technology companies such as Netflix, Uber, Airbnb, and Spotify in their production ML systems. This alignment with industry standards ensures that the methodologies and architectural patterns demonstrated here are directly transferable to real-world financial fraud detection deployments.

In summary, this work delivers not merely a high-performing fraud detection model, but a complete, observable, and maintainable ML system that embodies the principles of reliability, reproducibility, and continuous improvement central to modern MLOps engineering. The combination of thoughtful algorithmic design, comprehensive automation, and proactive monitoring creates a foundation for deploying machine learning safely and effectively in high-stakes production environments.

\subsection*{Future Work}

While the current system demonstrates strong performance and robust engineering, several avenues for enhancement remain. TabNet's attention-based architecture could be further optimized for production use, potentially offering improved interpretability through its feature selection masks. Advanced drift detection techniques such as ADWIN (Adaptive Windowing) or Page-Hinkley tests could provide more sensitive and responsive alerts. Integration with automated retraining pipelines triggered by drift thresholds would close the feedback loop entirely, enabling truly autonomous model lifecycle management. Finally, A/B testing frameworks could be incorporated to safely validate model updates in production before full rollout, minimizing the risk of performance regressions.

\section*{Appendices}

\subsection*{A. How to Reproduce This Project}

\begin{lstlisting}[language=bash]
# 1. Clone repository
git clone https://github.com/ainhoupna/MLOPS-FinalProject.git
cd MLOPS-FinalProject

# 2. Download dataset
kaggle datasets download -d mlg-ulb/creditcardfraud \
    -p data/raw --unzip

# 3. Install dependencies
pip install -r requirements.txt

# 4. Preprocess data
python -c "from src.data.preprocessing import DataPreprocessor; \
    DataPreprocessor().preprocess_pipeline()"

# 5. Train model
python src/models/train.py

# 6. View MLFlow results
mlflow ui --backend-store-uri mlruns

# 7. Run API
uvicorn src.api.main:app --reload --port 8000

# 8. Launch monitoring stack
cd docker && docker compose up -d
\end{lstlisting}

\subsection*{B. Environment Specifications}

\textbf{Python}: 3.10\\
\textbf{Key dependencies}:
\begin{itemize}
    \item xgboost==2.0.0
    \item optuna==3.5.0
    \item mlflow==2.10.0
    \item fastapi==0.109.0
    \item scikit-learn==1.4.0
    \item prometheus-client==0.19.0
\end{itemize}

\subsection*{C. Contact Information}

\textbf{Author}: Ainhoa Pina\\
\textbf{GitHub}: \url{https://github.com/ainhoupna}\\s
\textbf{Hugging Face}: \url{https://huggingface.co/ainhoupna}

\vfill
\noindent\rule{\textwidth}{0.4pt}\\
\textit{Report generated: January 3, 2026}\\
\textit{Project duration: December 2024 - January 2026}\\
\textit{Course: MLOps - Master in Data Science}

\end{document}


\end{document}